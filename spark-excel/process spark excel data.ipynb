{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pyspark excel processing using crealytics package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('D:\\spark-2.3.3-bin-hadoop2.7')\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder \\\n",
    "     .appName('Spark_DB') \\\n",
    "     .config(\"spark.jars.packages\", \"com.crealytics:spark-excel_2.11:0.12.2\") \\\n",
    "     .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-5c6b4d5d9db0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+---------+---------+-----+-------------+\n",
      "|EMPLOYEE_ID|    FNAME|    LNAME|HIRE_DATE|  SAL|DEPARTMENT_ID|\n",
      "+-----------+---------+---------+---------+-----+-------------+\n",
      "|        100|   Steven|     King|17-Jun-03|24000|           90|\n",
      "|        101|    Neena|  Kochhar|21-Sep-05|17000|           90|\n",
      "|        102|      Lex|  De Haan|13-Jan-01|17000|           90|\n",
      "|        103|Alexander|   Hunold| 3-Jan-06| 9000|           60|\n",
      "|        104|    Bruce|    Ernst|21-May-07| 6000|           60|\n",
      "|        105|    David|   Austin|25-Jun-05| 4800|           60|\n",
      "|        106|    Valli|Pataballa| 5-Feb-06| 4800|           60|\n",
      "|        107|    Diana|  Lorentz| 7-Feb-07| 4200|           60|\n",
      "|        108|    Nancy|Greenberg|17-Aug-02|12008|          100|\n",
      "|        109|   Daniel|   Faviet|16-Aug-02| 9000|          100|\n",
      "+-----------+---------+---------+---------+-----+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df=spark.read \\\n",
    "    .format(\"com.crealytics.spark.excel\") \\\n",
    "    .option(\"dataAddress\", \"'Employees'!A1\") \\\n",
    "    .option(\"useHeader\", \"true\") \\\n",
    "    .load(\"D:\\python_coding\\pyspark_tutorial\\demo_excel.xlsx\")\n",
    "\n",
    "emp_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------------+----------+-----------+\n",
      "|DEPARTMENT_ID| DEPARTMENT_NAME|MANAGER_ID|LOCATION_ID|\n",
      "+-------------+----------------+----------+-----------+\n",
      "|           10|  Administration|       200|       1700|\n",
      "|           20|       Marketing|       201|       1800|\n",
      "|           30|      Purchasing|       114|       1700|\n",
      "|           40| Human Resources|       203|       2400|\n",
      "|           50|        Shipping|       121|       1500|\n",
      "|           60|              IT|       103|       1400|\n",
      "|           70|Public Relations|       204|       2700|\n",
      "|           80|           Sales|       145|       2500|\n",
      "|           90|       Executive|       100|       1700|\n",
      "|          100|         Finance|       108|       1700|\n",
      "+-------------+----------------+----------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dept_df=spark.read \\\n",
    "    .format(\"com.crealytics.spark.excel\") \\\n",
    "    .option(\"dataAddress\", \"'Departments'!A1\") \\\n",
    "    .option(\"useHeader\", \"true\") \\\n",
    "    .load(\"D:\\python_coding\\pyspark_tutorial\\demo_excel.xlsx\")\n",
    "\n",
    "dept_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+\n",
      "|REGION_ID|         REGION_NAME|\n",
      "+---------+--------------------+\n",
      "|        1|              Europe|\n",
      "|        2|            Americas|\n",
      "|        3|                Asia|\n",
      "|        4|Middle East and A...|\n",
      "+---------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "region_df=spark.read \\\n",
    "    .format(\"com.crealytics.spark.excel\") \\\n",
    "    .option(\"dataAddress\", \"'Regions'!F5:G9\") \\\n",
    "    .option(\"useHeader\", \"true\") \\\n",
    "    .load(\"D:\\python_coding\\pyspark_tutorial\\demo_excel.xlsx\")\n",
    "\n",
    "region_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#join emp and dept datasets and create an excel output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+----------+---------+-----+-------------+-------------+----------------+----------+-----------+\n",
      "|EMPLOYEE_ID|      FNAME|     LNAME|HIRE_DATE|  SAL|DEPARTMENT_ID|DEPARTMENT_ID| DEPARTMENT_NAME|MANAGER_ID|LOCATION_ID|\n",
      "+-----------+-----------+----------+---------+-----+-------------+-------------+----------------+----------+-----------+\n",
      "|        114|        Den|  Raphaely| 7-Dec-02|11000|           30|           30|      Purchasing|       114|       1700|\n",
      "|        115|  Alexander|      Khoo|18-May-03| 3100|           30|           30|      Purchasing|       114|       1700|\n",
      "|        116|     Shelli|     Baida|24-Dec-05| 2900|           30|           30|      Purchasing|       114|       1700|\n",
      "|        117|      Sigal|    Tobias|24-Jul-05| 2800|           30|           30|      Purchasing|       114|       1700|\n",
      "|        118|        Guy|    Himuro|15-Nov-06| 2600|           30|           30|      Purchasing|       114|       1700|\n",
      "|        119|      Karen|Colmenares|10-Aug-07| 2500|           30|           30|      Purchasing|       114|       1700|\n",
      "|        205|    Shelley|   Higgins| 7-Jun-02|12008|          110|          110|      Accounting|       205|       1700|\n",
      "|        206|    William|     Gietz| 7-Jun-02| 8300|          110|          110|      Accounting|       205|       1700|\n",
      "|        108|      Nancy| Greenberg|17-Aug-02|12008|          100|          100|         Finance|       108|       1700|\n",
      "|        109|     Daniel|    Faviet|16-Aug-02| 9000|          100|          100|         Finance|       108|       1700|\n",
      "|        110|       John|      Chen|28-Sep-05| 8200|          100|          100|         Finance|       108|       1700|\n",
      "|        111|     Ismael|   Sciarra|30-Sep-05| 7700|          100|          100|         Finance|       108|       1700|\n",
      "|        112|Jose Manuel|     Urman| 7-Mar-06| 7800|          100|          100|         Finance|       108|       1700|\n",
      "|        113|       Luis|      Popp| 7-Dec-07| 6900|          100|          100|         Finance|       108|       1700|\n",
      "|        204|    Hermann|      Baer| 7-Jun-02|10000|           70|           70|Public Relations|       204|       2700|\n",
      "+-----------+-----------+----------+---------+-----+-------------+-------------+----------------+----------+-----------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_dept_df= emp_df.join(dept_df,emp_df.DEPARTMENT_ID==dept_df.DEPARTMENT_ID,how='inner')\n",
    "emp_dept_df.show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'emp_dept_df.write       .format(\"com.crealytics.spark.excel\")       .option(\"dataAddress\", \"\\'emp_dept\\'\")       .option(\"useHeader\", \"true\")       .mode(\"overwrite\")       .save(\"D:\\\\python_coding\\\\pyspark_tutorial\\\\output_demo.xlsx\")'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''emp_dept_df.write \\\n",
    "      .format(\"com.crealytics.spark.excel\") \\\n",
    "      .option(\"dataAddress\", \"'emp_dept'\") \\\n",
    "      .option(\"useHeader\", \"true\") \\\n",
    "      .mode(\"overwrite\") \\\n",
    "      .save(\"D:\\python_coding\\pyspark_tutorial\\output_demo.xlsx\")'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_dept_df.write.format(\"com.crealytics.spark.excel\") \\\n",
    "    .option(\"dataAddress\", \"emp_dept [#All]\") \\\n",
    "    .option(\"useHeader\", \"true\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .save(\"D:\\python_coding\\pyspark_tutorial\\output_demo.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "region_df.write.format(\"com.crealytics.spark.excel\") \\\n",
    "    .option(\"dataAddress\", \"region_details [#All]\") \\\n",
    "    .option(\"useHeader\", \"true\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .save(\"D:\\python_coding\\pyspark_tutorial\\output_demo.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "    df.write\n",
    "      .format(\"com.crealytics.spark.excel\")\n",
    "      .option(\"sheetName\", \"Daily\")\n",
    "      .option(\"useHeader\", \"true\")\n",
    "      .option(\"dateFormat\", \"yy-mmm-d\") // Optional, default: yy-m-d h:mm\n",
    "      .option(\"timestampFormat\", \"mm-dd-yyyy hh:mm:ss\") // Optional, default: yyyy-mm-dd hh:mm:ss.000\n",
    "      .mode(\"overwrite\")\n",
    "      .save(\"Worktime2.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
